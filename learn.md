# Things to learn

1. Naive Bayes Classifier
2. SVM
3. Nb -SVM algo : This is actually an SVM (Support Vector Machine) with Naive Bayes’ features. It creates a simple model variant where an SVM is built over NB log-count ratio “r” as feature values. 
The log form is an attempt to linearise the NB. Thus, in NB-SVM , the original features in the SVM, which are in vector form, are converted into a scalar by a scalar/dot product multiplication of each element with the log-count ratio ‘r’.
4. Autograd based differentiation -- PyTorch and TensorFlow
5. Decision Tree
6. Random Forest
7. Why writing is better https://addyosmani.com/blog/write-learn/
8. Fastai library https://docs.fast.ai/
9. Lightning library https://lightning.ai/docs/pytorch/stable/

## Discriminative ML

1. https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/

## DNN links for Tasks:

1. https://www.kaggle.com/code/dwayne99/3-sentiment-analysis-fasttext-in-pytorch
2. https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb
3. https://debuggercafe.com/saving-and-loading-the-best-model-in-pytorch/
4. Read about embedding layer work in nn https://thu-coai.github.io/cotk_docs/notes/tutorial_core.html
5. https://www.kaggle.com/code/houssemayed/lstm-models-for-regression-on-text-data#LSTM-regression-model
6. pytorch important parts https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/
7. https://faculty.cc.gatech.edu/~isbell/reading/papers/maxenttext.pdf
8. https://www.cs.jhu.edu/~jason/papers/jurafsky+martin.bookdraft07.ch6.pdf
9. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

## code to check

1. https://github.com/ncpitrsb/MaxEnt/blob/main/maxent.py maxent code
2. fake text data generation https://github.com/joke2k/faker
3. logistic vs maxent https://ataspinar.com/2016/05/07/regression-logistic-regression-and-maximum-entropy-part-2-code-examples/
4. maxent and lr equivalence https://github.com/WinVector/Examples/blob/main/dfiles/LogisticRegressionMaxEnt.pdf

## Intrepretable AI

1. https://christophm.github.io/interpretable-ml-book/influential.html

## LLM

1. https://lmql.ai/
2. https://finbarr.ca/how-is-llama-cpp-possible/
3. http://gltr.io/
4. https://thegradient.pub/othello/
5. https://www.ruder.io/state-of-transfer-learning-in-nlp/
6. https://kipp.ly/transformer-inference-arithmetic/
7. https://www.latent.space/p/agents
8. https://towardsdatascience.com/almost-no-data-and-no-time-unlocking-the-true-potential-of-gpt3-a-case-study-b4710ca0614a
9. https://jalammar.github.io/illustrated-bert/
10. https://peterbloem.nl/blog/transformers

## Good blogs

1. https://desh2608.github.io/blog/ for NLP
2. https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html
3. https://wandb.ai/shweta/Activation%20Functions/reports/Activation-Functions-Compared-With-Experiments--VmlldzoxMDQwOTQ#the-relu-activation-function
4. https://drive.google.com/file/d/1wzHohvoSgKGZvzOWqZybjm4M4veKR6t3/view (lecun presentation)
5. https://win-vector.com/2012/08/23/how-robust-is-logistic-regression/
6. https://amitness.com/archives/

## Bayesian ML

1. https://brunomaga.github.io/Bayesian-Linear-Regression/
2. https://sebastianraschka.com/Articles/2014_naive_bayes_1.html
3. https://spinningup.openai.com/en/latest/
4. https://distill.pub/2019/visual-exploration-gaussian-processes/
5. https://gregorygundersen.com/blog/2020/02/04/bayesian-linear-regression/
6. https://ermongroup.github.io/cs228-notes/
7. https://ermongroup.github.io/cs228-notes/representation/undirected/
8. https://betanalpha.github.io/assets/case_studies/probability_theory.html#1_setting_a_foundation
9. https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods
10. https://www.countbayesie.com/books
11. https://mlg.eng.cam.ac.uk/zoubin/bayesian.html
12. https://las.inf.ethz.ch/teaching/pai-f22
13. https://tomekkorbak.com/2020/05/29/interpreting-uncertainty-in-bayesian-linear-regression/

## Book links to go through

1. http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function
2. https://www.deeplearningbook.org/contents/mlp.html
3. https://course.fast.ai/
4. https://course.fast.ai/Resources/book.html

## Video links

1. https://www.youtube.com/watch?v=cXiqYer7nk0 (LR good one)
2. https://www.youtube.com/watch?v=PaCmpygFfXo&t=7s RNN step by step
3. https://www.youtube.com/watch?v=yj-wSRJwrrc using AI to understand user command or get data from unstructured text

## Courses links

1. https://rycolab.io/classes/llm-s23/

## Good writing tips

1. https://namedtensor.github.io/

## NLP

1. https://blog.echen.me/
2. https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45
3. https://github.com/ujjwalkarn/Machine-Learning-Tutorials#rnn
4. https://arxiv.org/pdf/1211.5063.pdf difficulty in trainig RNN
5. https://github.com/hans/glove.py/blob/582549ddeeeb445cc676615f64e318aba1f46295/glove.py
6. https://blog.floydhub.com/when-the-best-nlp-model-is-not-the-best-choice/
7. https://www.foldl.me/2014/glove-python/
8. https://github.com/dcetin/eth-cs-notes
9. https://arxiv.org/pdf/1506.03099.pdf rnn sampling
10. https://arxiv.org/pdf/1411.2738.pdf word2vec explained

## Statistics Concept

1. Variance inflation factor (VIF) statistics ruled out any potential multicollinearity issues in both datasets. Table 2 reports the mean, standard deviation, and correlation of variables.
