# [FastText](https://arxiv.org/abs/1607.04606)

Their key insight was to use the internal structure of a word to improve vector representations obtained from the skip-gram method.

They modified skip-gram by treating each word as the sum of its subword n-grams.
[FastText Good](https://amitness.com/2020/06/fasttext-embeddings/)

## Subword Generation

For a word, n-grams of length 3 to 6 are chosen. Lets take a word 'eating:

- add angular brackets to denote the beginning and end of a word

$\text{eating} \to \langle \text{eating} \rangle $

- For the word “eating”, character n-grams of length 3 can be generated by sliding a window of 3 characters from the start of the angular bracket till the ending angular bracket is reached. Here, we shift the window one step each time.

- We get the following character n-grams of length 3: <ea, eat, ati, tin, ing, ng>
- Similarly, we can generate character n-grams of length 4, 5 and 6.
- Since there can be huge number of unique n-grams, we apply hashing to bound the memory requirements. Instead of learning an embedding for each unique n-gram, we learn total B embeddings where B denotes the bucket size. The paper used a bucket of a size of 2 million.
- Each character n-gram is hashed to an integer between 1 to B. Though this could result in collisions, it helps control the vocabulary size. The paper uses the FNV-1a variant of the [Fowler-Noll-Vo hashing](http://www.isthe.com/chongo/tech/comp/fnv/) function to hash character sequences to integer values.

## PreTraining with SkipGram

We have a sentence with a center word “eating” and need to predict the context words “am” and “food”.
<img src="https://amitness.com/images/fasttext-toy-example.png" alt="sentence"/>

- The embedding for the center word is calculated by taking a sum of vectors for the character n-grams and the whole word itself.
<img src="https://amitness.com/images/fasttext-center-word-embedding.png" alt="center_embedding"/>

- For the actual context words, we directly take their word vector from the embedding table without adding the character n-grams.
- Now, we collect negative samples randomly with probability proportion to the square root of the unigram frequency. For one actual context word, 5 random negative words are sampled.
- We take dot product between the center word and the actual context words and apply sigmoid function to get a match score between 0 and 1.
- Based on the loss, we update the embedding vectors with SGD optimizer to bring actual context words closer to the center word but increase distance to the negative samples.
<img src="https://amitness.com/images/fasttext-negative-sampling-goal.png" alt="center_embedding"/>

## Advantages of FastText

- FastText can learn representations for out of vocabulary words.
- FastText can learn representations for rare words.
- FastText improves performance on syntactic word analogy tasks significantly for morphologically rich language like Czech and German.
- FastText word vectors are able to represent multiple words at the same time.
- Using sub-word information with character-ngrams has better performance than CBOW and skip-gram baselines on word-similarity task.

## Limitations of FastText

- FastText is not able to capture long-range dependencies between words directly.
- FastText is not able to capture relationships between words which are not co-occurring.
- FastText has degraded performance on semantic analogy tasks compared to Word2Vec.
- FastText is 1.5 times slower to train than regular skipgram due to added overhead of n-grams.
