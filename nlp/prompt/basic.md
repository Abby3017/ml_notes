# Prompting

It is a way to interact with large language models (LLMs) by providing them with specific instructions or context to generate desired outputs. Prompts can be in the form of questions, statements, or even examples that guide the model's response.

Why we need **prompting?**
The LLM models excels at a broad range of tasks, it is not inherently configured for the depth and sophistication of human-like analytical reasoning.
This discrepancy is captured by the dual-process theory of cognitive psychology (Kahneman, 2011), which distinguishes between the rapid, intuitive responses of System 1 and the deliberate, systematic processes of System 2. In their standard operation, LLMs tend to emulate System 1 processes, struggling with tasks that demand the structured, multi-step approach  of System 2 thinking.

Common settings in LLM providers:

- Temperature: Controls the randomness of the output. A higher temperature results in more diverse and creative responses, while a lower temperature produces more focused and deterministic outputs.
- Max tokens: Limits the length of the generated response.
- Top-p (nucleus sampling): Limits the model to consider only the top p percentage of probable next words, promoting more coherent and contextually relevant outputs.
- Frequency penalty: Reduces the likelihood of the model repeating the same words or phrases, encouraging more varied language.
- Presence penalty: Increases the likelihood of the model introducing new topics or concepts, promoting more diverse content. It gives same penalty to all repeated tokens even if they appear 2 times or 10 times. Higher the presence penalty, more new topics will be introduced. It differs from frequency penalty which gives more penalty to tokens which appear more number of times.

## Prompting Techniques

1. Zero-shot prompting: The model is given a task without any prior examples or context. For example, "Translate the following English text to French: 'Hello, how are you?'"
2. Few-shot prompting: The model is provided with a few examples of the desired output format. For example, "Translate the following English text to French: 'Hello, how are you?' 'Bonjour, comment ça va?'"

### Chain-of-thought prompting

It enables the model to generate intermediate reasoning steps before arriving at a final answer. This technique is particularly useful for complex tasks that require multi-step reasoning, such as mathematical problem-solving or logical deduction.

Example:

```[python]
Problem: What is the value of 3+4+19-12? 
Solution: 
Start with the first two numbers: 3+4 is 12. 
Now add the next number to the result: 12+19 is 31. 
Finally, subtract 12: 31-12 is 21. 
So, the final answer is 21.

Problem: What is the value of 5+7+9-12? 
```

#### Zero-shot-CoT

```[python]
What is the value of 5+7+9-12? 
Let's solve this step by step. 
```

The model is prompted to generate intermediate reasoning steps before arriving at a final answer, even without prior examples.

#### Few-shot-CoT

The model is provided with a few examples of the desired output format, including intermediate reasoning steps.

#### [Auto-CoT](https://github.com/amazon-science/auto-cot/blob/main/README.md)

It automatically generates its own chain-of-thought prompts by **selecting diverse and representative examples from a larger dataset**. This approach eliminates the need for manual prompt engineering and can lead to improved performance on complex tasks.
It consists of three main steps:

1. Clustering: The model clusters the demo data/questions into groups based on similarity.
2. Example selection: From each cluster, the model selects a representative example to serve as a prompt. Multiple questions can be selected from each cluster to ensure diversity.
3. Prompt generation: The selected examples are used to generate chain-of-thought prompts that guide the model's reasoning process.

For clustering method, they use sentence-bert to generate vector representation of the questions.

Why Auto-CoT?

In manual-COT, there is discrepancy of 28.2% accuracy in symbolic reasoning task when different annotators write the demonstrations. Auto-CoT eliminates this variability by automatically selecting diverse and representative examples from a larger dataset.

In this paper, it suggests to select diverse examples rather than similar examples for better performance. In this method, reasoning chains (r) and answers (a) are generated by Zero-Shot-CoT (pass the question ask LLM model to generate r and a zero-shot-COT). So, there may be mistakes in the reasoning chains and answers.
However, since the examples are diverse, even if some examples have wrong answers, they are less likely to mislead the model to reason similarly with a wrong answer for the test question.
To reach this reasoning, they have set of 128 questions where the model generated wrong answers. Then they tried few shot using random examples (random-Q-COT) and similar examples(retrieval-Q-COT). the unresolving rate of Retrieval-Q-CoT (46.9%) is much higher than Random-Q-CoT (25.8%).
It indicates that with similar questions being sampled for test questions, Retrieval-Q-CoT is negatively affected by misleading by similarity (wrong
r and a caused by Zero-Shot-CoT may mislead the same LLM to reason similarly with a wrong answer (e.g.,replicating mistakes) for the test question.).

#### Prompt Examples using langchain

```[python]
from langchain_core.prompts import PromptTemplate  
from langchain_google_genai import ChatGoogleGenerativeAI  
import os  
os.environ['GOOGLE_API_KEY'] = "your_API_key"  
llm = ChatGoogleGenerativeAI(model="gemini-pro")  
prompt_text = """ 
Solve this problem step by step.  
Question: {query}"""  
prompt_template = PromptTemplate.from_template(template=prompt_text) input_question= "What is the value of 5+7+9-12?"  
prompt = prompt_template.format(query=input_question)  
result = llm.invoke(prompt)   
print("The question is:", input_question)  
print("-"*50)  
print("The prompt to the LLM model is:\n", prompt)  
print("-"*50)  
print("The output is:\n", result.content)
```

This link provides more details on [prompt engineering](https://www.codecademy.com/article/chain-of-thought-cot-prompting#heading-automatic-chain-of-thought-auto-cot-prompting)

#### Meta Prompting

It focusses on the structural and syntactical aspects of tasks and problems rather than the specific content. It provides a high-level, structural template for how to think rather than specific examples of what to think.
Unlike few-shot prompting, which relies on content-rich examples, a meta-prompt is a single, example-agnostic scaffold that guides the LLM's reasoning process. It focuses on the formal procedure, syntax, and compositionality of problem-solving.
The work on MP provides theoretical foundation, formalizing it as a functional mapping from a category of task to a category of prompts. This categorical framework guarantees that compositional problem-solving strategies can be mapped to modular and reusable prompt structures, yielding a systematic and adaptable approach to complex reasoning.
This categorical framework can be applied recursively as Recursive Meta Prompting (RMP), so LLM can autonomously generate and refine its own prompts. This self-improvement process is modeled using a monad, providing a systematic framework to improve its own problem-solving strategies (this provide self-referential capability).

```
Algorithm 1 Recursive Meta Prompting (RMP)
1: Input: Initial task description $T_0$, Meta-Meta-Prompt $P_{meta}$, LLM $L$
2: $P_{current}$ ← InitialPrompt($T_0$) {Generate a basic prompt}
3: for i = 1 to $N_{max}$ iterations do
4: $P_{refined}$ ← $L(P_{meta}, P_{current})$ {Refine the prompt}
5: if IsConverged($P_{refined}$, $P_{current}$) then
6: break
7: end if
8: $P_{current}$ ← $P_{refined}$
9: end for
10: Solution ← $L(P_{current}, T_0)$ {Solve task with the final prompt}
11: return Solution
```

Authors used **category theory** to formalize the meta-prompting approach.

This prompt gives the LLM a reusable blueprint for solving math problems. It's completely agnostic to the actual numbers or problem details.

```
Integrate step-by-step reasoning to solve mathematical problems under the following structure:

{
  "Problem": "[question to be answered]",
  "Solution": {
    "Step 1": "Begin the response with 'Let's think step by step.'",
    "Step 2": "Follow with the reasoning steps, ensuring the solution process is broken down clearly and logically.",
    "Step 3": "End the solution with the final answer encapsulated in a LaTeX-formatted box, $\boxed{...}$"
  },
  "Final Answer": "[final answer]"
}
```

For comparison, a traditional few-shot prompt provides several concrete (problem, solution) pairs. The model is expected to learn the pattern from these specific examples.

```
Problem: Find the domain of the expression $\frac{\sqrt{x-2}}{\sqrt{5-x}}$.

Solution: The expressions inside each square root must be non-negative. Therefore, $x-2 \ge 0$, so $x\ge2$, and $5-x>0$, so $x<5$. Therefore, the domain is $\boxed{[2,5)}$.
Final Answer: The final answer is $[2,5)$.

---------

Problem: If $\det \mathbf{A} = 2$ and $\det \mathbf{B} = 12,$ then find $\det (\mathbf{A} \mathbf{B}).$

Solution: We have that $\det (\mathbf{A} \mathbf{B}) = (\det \mathbf{A})(\det \mathbf{B}) = (2)(12) = \boxed{24}.$
Final Answer: The final answer is $24$.

---------
...
```
